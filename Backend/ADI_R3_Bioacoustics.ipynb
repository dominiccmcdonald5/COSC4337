{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fkd5EovAGCzD","executionInfo":{"status":"ok","timestamp":1762372110820,"user_tz":360,"elapsed":4,"user":{"displayName":"Dylan Berens","userId":"00643723582163501960"}}},"outputs":[],"source":["# !apt-get install -y libsndfile1"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"M7HBEkqjEV0H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762372123983,"user_tz":360,"elapsed":13162,"user":{"displayName":"Dylan Berens","userId":"00643723582163501960"}},"outputId":"0bd7540d-308a-47ac-ae19-282c0dfbcc45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PySoundFile in /usr/local/lib/python3.12/dist-packages (0.9.0.post1)\n","Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.12/dist-packages (from PySoundFile) (2.0.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=0.6->PySoundFile) (2.23)\n","Requirement already satisfied: scikit-maad in /usr/local/lib/python3.12/dist-packages (1.5.1)\n","Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from scikit-maad) (2.0.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.12/dist-packages (from scikit-maad) (1.16.3)\n","Requirement already satisfied: scikit-image>=0.23.1 in /usr/local/lib/python3.12/dist-packages (from scikit-maad) (0.25.2)\n","Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.12/dist-packages (from scikit-maad) (2.2.2)\n","Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.12/dist-packages (from scikit-maad) (3.10.0)\n","Requirement already satisfied: pywavelets>=1.4 in /usr/local/lib/python3.12/dist-packages (from scikit-maad) (1.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->scikit-maad) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5->scikit-maad) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5->scikit-maad) (2025.2)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.23.1->scikit-maad) (3.5)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.23.1->scikit-maad) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.23.1->scikit-maad) (2025.10.16)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.23.1->scikit-maad) (0.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->scikit-maad) (1.17.0)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python-headless) (2.0.2)\n"]}],"source":["!pip install PySoundFile\n","!pip install scikit-maad\n","!pip install opencv-python-headless"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8OK35cDxVQP"},"outputs":[],"source":["# Core ML & GPU\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from torch.utils.data import Dataset\n","#import tensorflow_io as tfio\n","#import keras_cv\n","#from keras.callbacks import EarlyStopping\n","\n","# scikit-learn maad for when u want to grrr\n","from maad import sound, features\n","\n","# Huggingface Transformers\n","from transformers import AutoFeatureExtractor, TFViTModel #TFViTForImageClassification #TFViTModel #TFAutoModel #TFWhisperModel #, TFWhisperEncoder #TFWhisperModel\n","from transformers import AutoModelForAudioClassification\n","#from transformers.models.whisper.modeling_tf_whisper import TFWhisperEncoder\n","from huggingface_hub import login\n","\n","# Dataloading\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import librosa.display\n","import os\n","import glob #for finding TFRecord files\n","\n","# Colab specific\n","from google.colab import userdata\n","\n","from tqdm.auto import tqdm\n","\n","# Utilities\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eA_k2Hkc6M8t"},"outputs":[],"source":["# MOUNT GOOGLE DRIVE\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#!cp -r \"/content/drive/My Drive/tfrecords\" /content/\n","# !cp -r \"/content/drive/MyDrive/rfcx-species-audio-detection\" /content/\n","#print(\"rfcx-species-audio-detection copied successfully.\")\n","# drive.flush_and_unmount()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npkv3jscLkvU"},"outputs":[],"source":["# UNZIP FILE FROM DRIVE INTO DRIVES /content/ unzipped\n","\n","ZIP_FILE_PATH = \"/content/drive/MyDrive/rfcx-species-audio-detection.zip\"\n","\n","UNZIP_DESTINATION = \"/content/rfcx_local_data/\"\n","\n","print(f\"Unzupping {ZIP_FILE_PATH} to {UNZIP_DESTINATION} . . . \")\n","os.makedirs(UNZIP_DESTINATION, exist_ok=True)\n","\n","!unzip -q -o \"{ZIP_FILE_PATH}\" -d \"{UNZIP_DESTINATION}\"\n","\n","print(\"I took the zip and I unzipped the zip so now the zip is unzipped and ready\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nI7SXm6CqDnv"},"outputs":[],"source":["# !cp -r \"/content/drive/MyDrive/rfcx-species-audio-detection\" /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWyG4YyLGdCu"},"outputs":[],"source":["# MODEL DEFINITION CELL\n","\n","#from transformers import ASTFeatureExtractor\n","#from transformers import TFASTModel\n","\n","model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n","#model_checkpoint = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n","num_species = 24\n","\n","print(\"Loading ViT feature extractor and BASE model . . . \")\n","feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n","base_model = TFViTModel.from_pretrained(model_checkpoint, from_pt=True)\n","\n","#feature_extractor = ASTFeatureExtractor.from_pretrained(model_checkpoint)\n","#base_model = TFASTModel.from_pretrained(model_checkpoint, from_pt=True)\n","\n","class BiodiversityModel(tf.keras.Model):\n","  def __init__(self, vit_base_model):\n","    super().__init__()\n","\n","    self.vit = vit_base_model\n","\n","    self.regressor = layers.Dense(1, activation='sigmoid', name='biodiversity_score')\n","\n","  def call(self, inputs, training=None, explain=False):\n","\n","    transposed_inputs = tf.transpose(inputs, perm=[0, 3, 1, 2])\n","\n","    # hidden_states = self.vit(\n","    #     pixel_values=transposed_inputs,\n","    #     training=training,\n","    # )\n","    vit_outputs = self.vit(\n","        pixel_values=transposed_inputs,\n","        training=training\n","    )\n","\n","    last_hidden_state = vit_outputs.last_hidden_state\n","\n","    cls_token_output = last_hidden_state[:, 0, :]\n","\n","    final_score = self.regressor(cls_token_output)\n","\n","    if explain:\n","      #last_hidden_state = vit_outputs.last_hidden_state\n","      return final_score, last_hidden_state\n","\n","    return final_score\n","\n","print('\\nBuilding the final model with the custom ViT layer . . .')\n","model = BiodiversityModel(base_model)\n","\n","model.vit.trainable = False\n","\n","model.compile(\n","    optimizer='adam',\n","    loss=tf.keras.losses.MeanAbsoluteError(),\n","    metrics=['mae']\n",")\n","\n","print('\\n Ayo son we got it, the ViT model is build correctly and im ready to pull it')\n","\n","dummy_input = tf.ones((1, 224, 224, 3))\n","model(dummy_input)\n","model.summary()"]},{"cell_type":"code","source":["def calculate_adi(audio, sr, n_fft=2048, hop_length=512):\n","  try:\n","\n","    # 1. Calculate the spectrogram (power)\n","    # need the power spectrogram (Sxx), not the log-mel spectrogram\n","    Sxx, freqs, _ = sound.spectrogram(audio, sr, n_fft=n_fft, hop_length=hop_length, window='hann', flim=(0, 12000))\n","\n","    # 2. Calculate ADI\n","    # focus only on traditional biophony band (2k-11k)\n","\n","    fmin = 2000.0\n","    fmax = 13000.0\n","\n","    n_bands = 10\n","\n","    adi_score = features.acoustic_diversity_index(\n","        Sxx,\n","        freqs,\n","        fmin=fmin,\n","        fmax=fmax,\n","        n_bands=n_bands,\n","        db_threshold=-50.0 # ignores background noise quieter than -50dB\n","    )\n","\n","    # maad funciton returns a value between 0 and log(n_bands)\n","    # normalize by dividing by log(n_bands) to get a 0-1 score\n","    normalized_adi = adi_score / np.log(n_bands)\n","\n","    return np.float32(normalized_adi)\n","\n","  except Exception as e:\n","    print(f\"Warning: Could not calculate ADI. Error {e}\")\n","    return np.float32(0.0)\n"],"metadata":{"id":"jnT7ob5PV74S"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCvMsDYHWWVY"},"outputs":[],"source":["# DATALOADING AND PREPROCESSING\n","\n","def load_and_process_from_path(file_path_tensor):\n","\n","  def _process_audio_file(path_bytes):\n","    audio_path = path_bytes.numpy().decode('utf-8')\n","\n","    try:\n","      audio, sr = librosa.load(audio_path, sr=16000, duration=60)\n","    except Exception as e:\n","      print(f\"Warning: Could not load {audio_path}. Error {e}\")\n","      audio = np.zeros(16000 * 60, dtype=np.float32)\n","      sr = 16000\n","\n","    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=16000, n_fft=2048, hop_length=512, n_mels=224)\n","    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","    log_mel_spectrogram = np.nan_to_num(log_mel_spectrogram)\n","\n","    raw_adi_score = calculate_adi(audio, sr)\n","\n","    spectrogram_rgb = np.stack((log_mel_spectrogram,) * 3, axis=-1)\n","\n","    resized_spectrogram_rgb = tf.image.resize(spectrogram_rgb, [224, 224]).numpy()\n","\n","    #spectrogram_rgb = np.stack((log_mel_spectrogram,) * 3, axis=-1)\n","    inputs = feature_extractor(images=resized_spectrogram_rgb, return_tensors='np', do_rescale=False, do_resize=False)\n","    pixel_values = np.squeeze(inputs['pixel_values'], axis=0)\n","    pixel_values = np.moveaxis(pixel_values, 0, -1)\n","\n","    #return pixel_values.astype(np.float32), np.float32(raw_aci_score)\n","\n","    return pixel_values.astype(np.float32), np.float32(raw_adi_score)\n","\n","  features, raw_adi = tf.py_function(\n","      func=_process_audio_file, inp=[file_path_tensor], Tout=[tf.float32, tf.float32]\n","  )\n","\n","  features.set_shape([224, 224, 3])\n","  raw_adi.set_shape([])\n","\n","  return features, raw_adi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJtXFrMqWnZb"},"outputs":[],"source":["BATCH_SIZE = 16\n","\n","DRIVE_FLAC_PATH = \"/content/rfcx_local_data/rfcx-species-audio-detection/train/\"\n","\n","print(f\"Loading files from: {DRIVE_FLAC_PATH}\")\n","all_flac_files = sorted(glob.glob(DRIVE_FLAC_PATH + \"*.flac\"))\n","\n","split_point = int(len(all_flac_files) * 0.8)\n","train_files = all_flac_files[:split_point]\n","val_files = all_flac_files[split_point:]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zT-cnoL812Q7"},"outputs":[],"source":["# NDSI Calibration pass\n","\n","import soundfile as sf\n","\n","# silent_audio = np.zeros(16000*60, dtype=np.float32)\n","# blank_path = \"/content/drive/MyDrive/rfcx-species-audio-detection/blank.flac\"\n","\n","# sf.write(blank_path, silent_audio, 16000)\n","# print(f\"Created new blank file at {blank_path}\")\n","\n","anchor_files = [\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/blank.flac\",\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/airport_baggage.flac\",\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/bowling_alley.flac\",\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/bus.flac\",\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/laundromat.flac\",\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/subway.flac\",\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/urban_park_birds.flac\",\n","    \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/wind_stairwell.flac\",\n","]\n","\n","calibration_file_list = train_files + anchor_files\n","\n","print(f\"Calibrating adi scores across {len(calibration_file_list)} total files . . .\")\n","\n","cal_paths_ds = tf.data.Dataset.from_tensor_slices(calibration_file_list)\n","cal_ds_for_adi_only = cal_paths_ds.map(load_and_process_from_path, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","\n","cal_ds_zipped = tf.data.Dataset.zip((cal_ds_for_adi_only, cal_paths_ds))\n","\n","print(\"Iterating and pairing adi scores with filenames . . .\")\n","adi_data = []\n","\n","for (features, label), path in tqdm(cal_ds_zipped, total=len(calibration_file_list)):\n","  adi_data.append((path.numpy().decode('utf-8'), label.numpy()))\n","\n","df_adi = pd.DataFrame(adi_data, columns=['filename', 'raw_adi'])\n","\n","print(\"\\n--- ADI Score Analysis ---\")\n","\n","pd.set_option('display.max_colwidth', None)\n","\n","print(df_adi.sort_values(by='raw_adi', ascending=False).head(5))\n","\n","adi_scores = df_adi['raw_adi'].values\n","\n","#aci_scores = [label.numpy() for features, label in tqdm(cal_ds_for_aci_only)]\n","\n","min_adi = np.min(adi_scores)\n","max_adi = np.max(adi_scores)\n","print(f\"Calibration done. Min ADI: {min_:.4f}, Max ADI: {max_adi:.4f}\")\n","\n","def normalize_label(features, raw_adi_label):\n","  normalized_label = (raw_adi_label - min_adi) / (max_adi - min_adi + 1e-10)\n","  normalized_label = tf.clip_by_value(normalized_label, 0.0, 1.0)\n","  return features, normalized_label\n","\n","print(\"Building training dataset (from Amazon files only) . . .\")\n","\n","#paths_ds = tf.data.Dataset.from_tensor_slices(train_files)\n","\n","train_paths_ds = tf.data.Dataset.from_tensor_slices(calibration_file_list)\n","\n","train_ds = (\n","    train_paths_ds.map(load_and_process_from_path, num_parallel_calls=tf.data.AUTOTUNE)\n","    .map(normalize_label, num_parallel_calls=tf.data.AUTOTUNE)\n","    .cache()\n","    .shuffle(1024)\n","    .batch(BATCH_SIZE)\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","\n","print(\"Buuilding validation dataset . . .\")\n","val_paths_ds = tf.data.Dataset.from_tensor_slices(val_files)\n","val_ds = (\n","    val_paths_ds.map(load_and_process_from_path, num_parallel_calls=tf.data.AUTOTUNE)\n","    .map(normalize_label, num_parallel_calls=tf.data.AUTOTUNE)\n","    .cache()\n","    .batch(BATCH_SIZE)\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","\n","print(\"\\n--- Final Dataset Test ---\")\n","for features, labels in train_ds.take(1):\n","  print(\"Data pipeline working with .flac files and normalized ADI (with Domain Adaptation)!\")\n","  print(\"Features shape:\", features.shape)\n","  print(\"Example Normalized ADI scores:\", labels.numpy()[:5])"]},{"cell_type":"code","source":["# PLOT THE DISTRIBUTION OF ADI SCORES ACROSS DATASET\n","\n","print(f\"Mean adi: {np.mean(adi_scores):.4f}\")\n","print(f\"Median adi: {np.median(adi_scores):.4f}\")\n","\n","plt.figure(figsize=(10, 6))\n","plt.hist(adi_scores, bins=100)\n","plt.title('Distribution of Raw adi Scores (Including Anchors)')\n","plt.xlabel('Raw adi Score')\n","plt.ylabel('Number of Files')\n","plt.axvline(np.mean(adi_scores), color='red', linestyle='dashed', linewidth=2, label=f'Mean: {np.mean(adi_scores):.4f}')\n","plt.axvline(np.median(adi_scores), color='green', linestyle='dashed', linewidth=2, label=f'Median: {np.median(adi_scores):.4f}')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"-Tf8TEW3c1Nb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywUzmtX4Y0Js"},"outputs":[],"source":["# TRAINING\n","\n","steps_per_epoch = -(-len(train_files) // BATCH_SIZE)\n","validation_steps = -(-len(val_files) // BATCH_SIZE)\n","\n","print(\"Starting initial training with the base model frozen . . .\")\n","history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=5,\n","    steps_per_epoch=steps_per_epoch,\n","    validation_steps=validation_steps\n","    #callbacks=[early_stopping]\n",")\n","\n","print(\"Section 1 training complete!\")\n","\n","# FINE TUNING\n","\n","print(\"\\nUnfreezing the ViT base model for fine-tuning . . .\")\n","model.vit.trainable = True\n","\n","lr_scheduler = ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.2,\n","    patience=5,\n","    verbose=1,\n","    min_lr=1e-7\n",")\n","\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","checkpoint_path = \"/content/drive/MyDrive/best_model.keras\"\n","\n","model_checkpoint = ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    monitor='val_loss',\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","learning_rate = 1e-3\n","\n","print(f\"Re-compiliing model with a much lower learning rate {learning_rate} . . . \")\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","    loss=tf.keras.losses.MeanAbsoluteError(),\n","    metrics=['mae']\n",")\n","\n","\n","print(\"\\nStarting fine-tuning . . .\")\n","fine_tune_history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=25,\n","    initial_epoch=history.epoch[-1] + 1,\n","    steps_per_epoch=steps_per_epoch,\n","    validation_steps=validation_steps,\n","    callbacks=[early_stopping, lr_scheduler, model_checkpoint]\n","    #callbacks=[early_stopping]\n",")\n","\n","print(\"Section 2 fine-tuning was another fat dub\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlxEqVwjY4tW"},"outputs":[],"source":["# VISUALIZE MEL AUDIO SPECTROGRAM FOR SINGLE FLAC TO CONFIRM NO ISSUES\n","\n","#path_to_audio_file = \"/content/000316da7.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/train/00d442df7.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/train/0072f0839.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/train/011f25080.flac\"\n","\n","# LETS BOWL\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/bowling_alley.flac\"\n","\n","# BLANK FILE\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/blank.flac\"\n","\n","# LOW SCORE\n","path_to_audio_file = \"/content/rfcx_local_data/rfcx-species-audio-detection/train/e48bf871c.flac\"\n","\n","# path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/blank.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/airport_baggage.flac\"\n","# path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/bowling_alley.flac\"\n","# path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/bus.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/laundromat.flac\"\n","# path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/subway.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/urban_park_birds.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/anchor/wind_stairwell.flac\"\n","\n","# GLITCHED AUDIO FILE, WAS THE HIGHEST SCORE BACK WHEN WE USED ACI\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/train/288e5d13f.flac\"\n","#path_to_audio_file = \"/content/7e14bfa8e.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/train/716dccfed.flac\"\n","\n","try:\n","  audio_np, sr = librosa.load(path_to_audio_file, sr=16000)\n","  print(\"Successfully loaded audio file\")\n","except Exception as e:\n","  print(f\"Error loading file: {e}\")\n","  audio_np = np.zeros(16000 * 60)\n","  sr = 16000\n","\n","mel_spectrogram = librosa.feature.melspectrogram(\n","    y=audio_np, sr=16000, n_fft=2048, hop_length=512, n_mels=224\n",")\n","\n","log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","log_mel_spectrogram = np.nan_to_num(log_mel_spectrogram)\n","\n","#biodiversity_score = calculate_aci(log_mel_spectrogram)\n","\n","#raw_aci_score = calculate_aci(log_mel_spectrogram)\n","\n","raw_adi_score = calculate_adi_manual(audio_np, sr)\n","\n","normalized_score = (raw_adi_score - min_adi) / (max_adi - min_adi + 1e-10)\n","normalized_score = np.clip(normalized_score, 0, 1)\n","\n","plt.figure(figsize=(12, 5))\n","librosa.display.specshow(\n","    log_mel_spectrogram,\n","    sr=16000,\n","    x_axis='time',\n","    y_axis='mel',\n","    fmax=8000\n","    #cmap='jet'\n",")\n","\n","\n","plt.colorbar(format='%+2.0f dB')\n","plt.title(f'Mel Spectrogram (Biodiversity Score: {normalized_score:.2f})')\n","plt.xlabel('Time (s)')\n","plt.ylabel('Frequency (Hz)')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qA620eq5G8i"},"outputs":[],"source":["TEST_FLAC_PATH = \"/content/rfcx_local_data/rfcx-species-audio-detection/test/\"\n","test_files = sorted(glob.glob(TEST_FLAC_PATH + \"*.flac\"))\n","\n","test_paths_ds = tf.data.Dataset.from_tensor_slices(test_files)\n","test_ds = (\n","    test_paths_ds.map(load_and_process_from_path, num_parallel_calls=tf.data.AUTOTUNE)\n","    .map(normalize_label, num_parallel_calls=tf.data.AUTOTUNE)\n","    .cache()\n","    .batch(BATCH_SIZE)\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","\n","results = model.evaluate(test_ds)\n","\n","print(\"\\n--- Test Results ---\")\n","print(f\"Test Loss (MAE): {results[0]:.4f}\")\n","print(f\"Test Metric (MAE): {results[1]:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_ino_Z96pIs"},"outputs":[],"source":["from sklearn.metrics import r2_score, mean_squared_error\n","\n","print(\"Running full prediction loop to get labels and predictions . . .\")\n","\n","y_pred_normalized = model.predict(val_ds)\n","\n","y_true_normalized = []\n","\n","for features, labels in val_ds.as_numpy_iterator():\n","  y_true_normalized.extend(labels)\n","\n","y_true_normalized = np.array(y_true_normalized)\n","y_pred_normalized = np.squeeze(y_pred_normalized)\n","\n","df_results = pd.DataFrame({\n","    'filename': val_files,\n","    'true_score': y_true_normalized,\n","    'pred_score': y_pred_normalized\n","})\n","\n","print(\"\\n--- Files with big boy scores:\")\n","print(df_results.sort_values(by='pred_score', ascending=False).head())\n","\n","print(\"\\n--- Files with teeny tiny scores:\")\n","print(df_results.sort_values(by='pred_score', ascending=False).tail())\n","\n","print(\"\\n--- Final Model Performance Metrics ---\")\n","\n","r2 = r2_score(y_true_normalized, y_pred_normalized)\n","print(f\"R-Squared (R2): {r2:.4f}\")\n","\n","rmse = np.sqrt(mean_squared_error(y_true_normalized, y_pred_normalized))\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n","print(f\"Mean Absolute Error (MAE): {np.mean(np.abs(y_true_normalized - y_pred_normalized)):.4f}\")\n","\n","print(\"\\nGenerating 'Predicted vs. Actual' Plot . . . \")\n","\n","plt.figure(figsize=(10, 8))\n","plt.scatter(y_true_normalized, y_pred_normalized, alpha=0.3)\n","\n","perfect_line = np.linspace(0, 1, 100)\n","plt.plot(perfect_line, perfect_line, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n","\n","plt.xlabel('True Normalized ADI (0-1 Scale)', fontsize=14)\n","plt.ylabel('Predicted Normalized ADI (0-1 Scale)', fontsize=14)\n","plt.title('Predicted vs. Actual Biodiversity Score', fontsize=18)\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKPoJOwc9Fdp"},"outputs":[],"source":["print(\"\\nGenerating 'Residuals' plot . . . \")\n","\n","residuals = y_true_normalized - y_pred_normalized\n","\n","plt.figure(figsize=(10, 6))\n","plt.scatter(y_pred_normalized, residuals, alpha=0.3)\n","\n","plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n","\n","plt.xlabel('Predicted Score', fontsize=14)\n","plt.ylabel('Residual (Error)', fontsize=14)\n","plt.title('Residuals Plot', fontsize=18)\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","source":["import tensorflow.keras.backend as K\n","import cv2\n","\n","def generate_grad_cam_heatmap(img_array, model):\n","\n","  # 1. Create new functional API model for explainability\n","  vit_base_model = model.vit\n","  regressor_head = model.regressor\n","\n","\n","  input_layer = tf.keras.Input(shape=(224, 224, 3))\n","\n","  transposed_inputs = tf.transpose(input_layer, perm=[0, 3, 1, 2])\n","\n","  vit_outputs = vit_base_model(pixel_values=transposed_inputs, training=False)\n","\n","  last_hidden_state = vit_outputs.last_hidden_state\n","  cls_token_output = last_hidden_state[:, 0, :]\n","\n","  final_score = regressor_head(cls_token_output)\n","\n","  cam_model = tf.keras.Model(inputs=input_layer, outputs=[final_score, last_hidden_state])\n","\n","  img_tensor = tf.convert_to_tensor(img_array)\n","\n","  with tf.GradientTape() as tape:\n","    final_score_pred, last_hidden_state_output = cam_model(img_tensor, training=False)\n","\n","  grads = tape.gradient(final_score_pred, last_hidden_state_output)\n","\n","  if grads is None:\n","    print(\"ERROR: Gradient is None. The functional model graph failed.\")\n","    return np.zeros((img_array.shape[1], img_array.shape[2]), dtype=np.uint8)\n","\n","  patch_grads = grads[:, 1:, :]\n","  heatmap = tf.reduce_mean(patch_grads, axis=-1)\n","  heatmap = tf.reshape(heatmap, (14, 14))\n","\n","  heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + K.epsilon())\n","  heatmap = heatmap.numpy()\n","  heatmap = cv2.resize(heatmap, (img_array.shape[2], img_array.shape[1]))\n","  heatmap = (heatmap * 255).astype(np.uint8)\n","\n","  return heatmap"],"metadata":{"id":"02tYetY71gkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxG9mHQk6xNA"},"outputs":[],"source":["# cell for backend prediction function\n","\n","import cv2\n","import io\n","import base64\n","\n","\n","def get_prediction_and_heatmap(audio_file_path, model, feature_extractor, min_adi, max_adi):\n","\n","  # 1. preprocess audio file\n","  audio, sr = librosa.load(audio_file_path, sr=16000, duration=60)\n","\n","  # get the label for display\n","  raw_score = calculate_adi_manual(audio, sr)\n","  normalized_score = (raw_score - min_adi) / (max_adi - min_adi + 1e-10)\n","  normalized_score = np.clip(normalized_score, 0, 1)\n","\n","  # get the image\n","  mel_spec = librosa.feature.melspectrogram(y=audio, sr=16000, n_fft=2048, hop_length=512, n_mels=224)\n","  log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n","  log_mel_spec_rgb = np.stack((log_mel_spec,) * 3, axis=-1)\n","\n","  # resize and use feature extractor to get (1, 3, 224, 224) input\n","  resized_spec = tf.image.resize(log_mel_spec_rgb, [224, 224]).numpy()\n","  inputs = feature_extractor(images=resized_spec, return_tensors='np', do_rescale=False, do_resize=False)\n","\n","  # this is the (1, 224, 224, 3) array for Grad-CAM\n","  img_array_for_cam = np.moveaxis(np.squeeze(inputs['pixel_values'], axis=0), 0, -1)\n","  img_array_for_cam = np.expand_dims(img_array_for_cam, axis=0)\n","\n","  # 2. get model prediction\n","  predicted_score = model.predict(img_array_for_cam)[0][0]\n","\n","  # 3. generate grad-CAM heatmap\n","  heatmap = generate_grad_cam_heatmap(img_array_for_cam, model)\n","\n","  # 4. create and encode overlay image\n","\n","  # matplotlib to create overlay\n","  fig, ax = plt.subplots()\n","\n","  # plot original log-mel spectrogram\n","  librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, ax=ax)\n","\n","  # overlay the heatmap, resied to match the original spec's dimensions\n","  heatmap_resized = cv2.resize(heatmap, (log_mel_spec.shape[1], log_mel_spec.shape[0]))\n","  ax.imshow(heatmap_resized, cmap='jet', alpha=0.5, aspect='auto')\n","\n","  ax.set_title(f\"Predicted Score: {predicted_score:.2f} (True Score: {normalized_score:.2f})\")\n","\n","  # save the plot to a buffer\n","  buf = io.BytesIO()\n","  plt.savefig(buf, format='png', bbox_inches='tight')\n","  plt.close(fig)\n","\n","  image_b64 = base64.b64encode(buf.getvalue().decode('uft-8'))\n","\n","  return {\n","      \"biodiversity_score\": float(predicted_score),\n","      \"true_score\": float(normalized_score),\n","      \"explainability_map_b64\": image_b64\n","      # also add plotly distribution plot\n","  }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qP1g5yPPKJrD"},"outputs":[],"source":["#!pip install opencv-python-headless\n","\n","import cv2\n","\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/train/00d442df7.flac\"\n","#path_to_audio_file = \"/content/drive/MyDrive/rfcx-species-audio-detection/train/288e5d13f.flac\"\n","\n","#high score\n","path_to_audio_file = \"/content/rfcx_local_data/rfcx-species-audio-detection/train/eb62e367c.flac\"\n","\n","# good effort score\n","#path_to_audio_file = \"/content/rfcx_local_data/rfcx-species-audio-detection/train/e48bf871c.flac\"\n","\n","\n","print(f\"Loading and processing: {path_to_audio_file}\")\n","\n","try:\n","  audio, sr = librosa.load(path_to_audio_file, sr=16000, duration=60)\n","except Exception as e:\n","  print(f\"Warning: Could not load {path_to_audio_file}. Error {e}\")\n","  audio = np.zeros(16000*60, dtype=np.float32)\n","\n","mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=16000, n_fft=2048, hop_length=512, n_mels=224)\n","log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","log_mel_spectrogram = np.nan_to_num(log_mel_spectrogram)\n","\n","spectrogram_rgb = np.stack((log_mel_spectrogram,) * 3, axis=-1)\n","resized_spectrogram_rgb = tf.image.resize(spectrogram_rgb, [224, 224]).numpy()\n","\n","inputs = feature_extractor(images=resized_spectrogram_rgb, return_tensors='np', do_rescale=False, do_resize=False)\n","pixel_values = np.squeeze(inputs['pixel_values'], axis=0)\n","pixel_values = np.moveaxis(pixel_values, 0, -1)\n","\n","img_array_for_cam = np.expand_dims(pixel_values, axis=0)\n","\n","print(\"Generating Grad-CAM heatmap . . . \")\n","heatmap = generate_grad_cam_heatmap(\n","    img_array_for_cam,\n","    model\n",")\n","\n","print(\"Plotting results . . .\")\n","\n","predicted_score = model.predict(img_array_for_cam)[0][0]\n","\n","fig, ax = plt.subplots(figsize=(14, 5))\n","\n","HOP_LENGTH = 512\n","SR = 16000\n","FMAX = 8000\n","duration_sec = (log_mel_spectrogram.shape[1]*HOP_LENGTH) / SR\n","\n","librosa.display.specshow(\n","    log_mel_spectrogram,\n","    sr=sr,\n","    x_axis='time',\n","    y_axis='mel',\n","    fmax=8000,\n","    ax=ax,\n","    cmap='magma',\n",")\n","\n","heatmap_resized = cv2.resize(heatmap, (log_mel_spectrogram.shape[1], log_mel_spectrogram.shape[0]))\n","\n","ax.imshow(\n","    heatmap_resized,\n","    cmap='jet',\n","    alpha=0.9,\n","    aspect='auto',\n","    extent=[0, duration_sec, 0, FMAX]\n",")\n","\n","ax.set_title(f\"Grad-CAM Heatmap (Predicted Score: {predicted_score:.2f})\")\n","mappable = ax.collections[0] if ax.collections else ax.images[0]\n","plt.colorbar(mappable, ax=ax, format=\"%+2.0f dB\")\n","# plt.colorbar(ax.collections[1], format=\"%+2.0f dB\")\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1HbfBpZLwnubu7rNt0K7Sql2Lo7j-NNbH","timestamp":1762368364351},{"file_id":"1JM3Fjcqciv6RxtfC26tpzJolA5HIRCQK","timestamp":1762309565915},{"file_id":"1T1qKazEgcEnXVRU6uzjR2WKkOkYyD4i8","timestamp":1761865457984}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}